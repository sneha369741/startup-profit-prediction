# -*- coding: utf-8 -*-
"""ProfitPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WyxYlrFnSoz3i8qAl8Kmzj3NGBF1d91q

1 Reading the data
"""

#importing basic and visualization libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.pipeline import Pipeline, make_pipeline

#importing statmodels
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor

#importing metrics
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, auc, roc_curve

#importing sklearn-dataprocessing
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.feature_selection import f_regression
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import PolynomialFeatures, StandardScaler, OneHotEncoder
from sklearn.metrics import r2_score, mean_squared_error

#importing sklearn-models
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.svm import SVR
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

import warnings
warnings.filterwarnings('ignore')

sns.set_theme(style='darkgrid', palette='Accent')
pd.options.display.float_format = '{:,.2f}'.format

startups= pd.read_csv("/content/50_Startups.csv")
startups.head()

startups.shape

startups.info()

startups.isnull().sum()

startups.duplicated().any()

startups.describe()

"""2 Exploratory Data Analysis (EDA)

---
Checking for outliers

"""

startups.columns.values

sns.boxplot(data=startups)

# We have an outlier in the Profit column. Since the dataset is small, this could be a problem in predicting the profit. Hence we need to remove this outlier

Q3, Q1 = np.percentile(startups["Profit"], [75 ,25])
IQR = Q3 - Q1
startups = startups[~(startups.Profit< (Q1 - 1.5*IQR))]

"""Visualizing numerical variables"""

sns.pairplot(startups[['R&D Spend', 'Administration', 'Marketing Spend', 'Profit']], kind="reg", diag_kind="kde")
plt.show()

"""Profit distribution"""

sns.distplot(startups["Profit"], bins=30)
plt.show()

#gives positive & negative relation between categories
sns.heatmap(startups.corr(), annot=True)

"""R&D spend versus Profit correlation"""

sns.jointplot(x=startups["Profit"], y=startups["R&D Spend"], kind="reg")
plt.show()

"""3 Data preparation"""

startups_prepared = startups.copy()

"""Check for multi collinearity"""

numerical = startups_prepared.drop(columns=["Profit"])
vif = pd.DataFrame()
vif["Features"] = numerical.columns
vif["VIF"] = [variance_inflation_factor(numerical.values, i) for i in range(numerical.shape[1])]
vif["VIF"] = round(vif["VIF"], 2)
vif = vif.sort_values(by = "VIF", ascending = False)
vif

"""*  VIF scores are higher for R&D and Marketing Spend.

*  Since Administration is not so correlated with Profit as other variables, we will consider dropping this variable, which will drive VIF factor down.

Creating dummy variables
"""

startups_prepared = pd.get_dummies(startups_prepared, drop_first=True)
startups_prepared.rename(columns={"R&D Spend":"R&D", "Marketing Spend":"Marketing"}, inplace=True)

startups_prepared.head()

"""Defining input and target variables"""

X = startups_prepared.drop(columns="Profit")
y = startups_prepared.Profit

"""Feature selection"""

data = f_regression(X[["R&D", "Administration", "Marketing"]], y)
f_df = pd.DataFrame(data, index=[["F_statistic", "p_value"]], columns=X[["R&D", "Administration", "Marketing"]].columns).T
f_df

"""

*   R&D and Marketing has nearly 0 p-value what implies statistical significance.
*   On the other hand Administration seems to have no effect in predicting the Profit, as we previously seen from correlation as well.
We are going to drop Administration column as it has no statistical significance in our model.


"""

X = X.drop(columns="Administration")

"""Splitting the data into training and testing sets"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=6)

"""Scaling the features"""

#scaling inputs
sc_x = StandardScaler()
X_train = sc_x.fit_transform(X_train)
X_test = sc_x.transform(X_test)

#scaling target variable
sc_y = StandardScaler()
y_train = sc_y.fit_transform(y_train.values.reshape(-1, 1))
y_test = sc_y.transform(y_test.values.reshape(-1, 1))

y_train = y_train.reshape(44)
y_test = y_test.reshape(5)

"""4 Model selection and evaluation

---
Multiple Linear Regression

"""

Rsqr_test = []
order = range(1,4)
for n in order:
    pr = PolynomialFeatures(degree=n)
    X_train_poly = pr.fit_transform(X_train)
    X_test_poly = pr.fit_transform(X_test)
    lr = LinearRegression()
    lr.fit(X_train_poly, y_train)
    Rsqr_test.append(lr.score(X_test_poly, y_test))
Rsqr_test

lr = LinearRegression()
lr.fit(X_train, y_train)
r2_score = lr.score(X_test, y_test)

print(f"Training Accuracy Score: {lr.score(X_train, y_train) * 100:.1f}%")
print(f"Validation Accuracy Score: {lr.score(X_test, y_test) * 100:.1f}%")

plt.plot(Rsqr_test)
plt.show()

"""Support Vector Regression"""

svr = SVR()
svr.fit(X_train, y_train)
print(f"Training Accuracy Score: {svr.score(X_train, y_train) * 100:.1f}%")
print(f"Validation Accuracy Score: {svr.score(X_test, y_test) * 100:.1f}%")

"""Decision Tree Regression"""

dt = DecisionTreeRegressor()
dt.fit(X_train, y_train)
print(f"Training Accuracy Score: {dt.score(X_train, y_train) * 100:.1f}%")
print(f"Validation Accuracy Score: {dt.score(X_test, y_test) * 100:.1f}%")

"""Random Forest Regression"""

rf = RandomForestRegressor()
rf.fit(X_train, y_train)
print(f"Training Accuracy Score: {rf.score(X_train, y_train) * 100:.1f}%")
print(f"Validation Accuracy Score: {rf.score(X_test, y_test) * 100:.1f}%")

"""Thus the best performing model is Multiple Regression"""

r2_score

#adjusted R-square of the model
n = X_test.shape[0]
p = X_test.shape[1]

adjusted_r2 = 1-(1-r2_score)*(n-1)/(n-p-1)
adjusted_r2

"""5 Residual Analysis"""

y_test_hat = lr.predict(X_test)
y_test_hat

plt.scatter(x=y_test, y=y_test_hat, alpha=0.8)
plt.plot(y_test, y_test, color='darkgreen')
plt.show()

residuals = y_test - y_test_hat

plt.scatter(y=residuals, x=y_test_hat, alpha=0.8)
plt.show()

lr.coef_

lr.intercept_

#individual prediction
lr.predict([[100671.96, 249744.55]])

#individual prediction
lr.predict([[130298.1, 323876.68]])

#individual prediction
lr.predict([[131876.9, 362861.36]])

#individual prediction
lr.predict([[77044.01, 140574.81]])